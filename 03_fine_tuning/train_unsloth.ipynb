{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Campus_GPT Fine-tuning with Unsloth (Local GPU)\n",
                "\n",
                "This notebook fine-tunes Llama 3.1 8B on the generated RAFT dataset using your local GPU.\n",
                "\n",
                "**Requirements:**\n",
                "- CUDA-capable GPU (12GB+ VRAM recommended)\n",
                "- Python 3.10+\n",
                "- CUDA Toolkit installed\n",
                "\n",
                "**Setup:**\n",
                "```bash\n",
                "# Install dependencies with uv\n",
                "uv pip install \"unsloth[cu124] @ git+https://github.com/unslothai/unsloth.git\"\n",
                "uv pip install transformers==4.56.2 trl==0.22.2 datasets accelerate bitsandbytes\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Model with 4-bit Quantization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "\n",
                "max_seq_length = 4096  # Supports RoPE Scaling\n",
                "dtype = None  # Auto-detect (Float16 for older GPUs, Bfloat16 for Ampere+)\n",
                "load_in_4bit = True  # 4-bit quantization to fit in 12GB VRAM\n",
                "\n",
                "print(\"Loading Llama 3.1 8B with 4-bit quantization...\")\n",
                "\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
                "    max_seq_length=max_seq_length,\n",
                "    dtype=dtype,\n",
                "    load_in_4bit=load_in_4bit,\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Model loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define RAFT Prompt Template\n",
                "\n",
                "This template includes:\n",
                "- **Context**: Oracle + Distractors\n",
                "- **Question**: User query\n",
                "- **Thought Process**: Chain-of-Thought reasoning\n",
                "- **Answer**: Final response"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Llama 3.1 format with thinking tags\n",
                "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
                "\n",
                "You are Campus_GPT, an AI assistant for Northern Kentucky University. \n",
                "Use the provided context to answer questions. Show your reasoning before the final answer.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
                "\n",
                "Context: {context}\n",
                "Question: {instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
                "\n",
                "<|thought|>\n",
                "{thought}\n",
                "<|answer|>\n",
                "{answer}<|eot_id|>\"\"\"\n",
                "\n",
                "EOS_TOKEN = tokenizer.eos_token\n",
                "\n",
                "\n",
                "def formatting_prompts_func(examples):\n",
                "    \"\"\"\n",
                "    Map RAFT dataset to Llama 3.1 format.\n",
                "    \n",
                "    RAFT keys:\n",
                "    - question: User query\n",
                "    - oracle: The correct context chunk\n",
                "    - distractors: List of similar but incorrect chunks\n",
                "    - thought_process: Chain-of-Thought reasoning\n",
                "    - answer: Final answer\n",
                "    \"\"\"\n",
                "    instructions = examples[\"question\"]\n",
                "    oracles = examples[\"oracle\"]\n",
                "    distractors_list = examples[\"distractors\"]\n",
                "    thoughts = examples[\"thought_process\"]\n",
                "    answers = examples[\"answer\"]\n",
                "    \n",
                "    texts = []\n",
                "    for instruction, oracle, distractors, thought, answer in zip(\n",
                "        instructions, oracles, distractors_list, thoughts, answers\n",
                "    ):\n",
                "        # Combine Oracle + Distractors\n",
                "        if isinstance(distractors, list):\n",
                "            all_contexts = [oracle] + distractors[:3]  # Oracle + 3 distractors\n",
                "        else:\n",
                "            all_contexts = [oracle]\n",
                "        \n",
                "        context_str = \"\\n\\n\".join([f\"Document {i+1}:\\n{c}\" for i, c in enumerate(all_contexts)])\n",
                "        \n",
                "        text = prompt_template.format(\n",
                "            context=context_str,\n",
                "            instruction=instruction,\n",
                "            thought=thought,\n",
                "            answer=answer\n",
                "        ) + EOS_TOKEN\n",
                "        texts.append(text)\n",
                "    \n",
                "    return {\"text\": texts}\n",
                "\n",
                "\n",
                "print(\"‚úÖ Prompt template defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load RAFT Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "import os\n",
                "\n",
                "# Direct path to raft_dataset.jsonl in 03_fine_tuning\n",
                "dataset_path = \"raft_dataset.jsonl\"\n",
                "\n",
                "if not os.path.exists(dataset_path):\n",
                "    raise FileNotFoundError(\n",
                "        f\"‚ùå {dataset_path} not found!\\n\"\n",
                "        \"Make sure you've run: python generate_raft_focused.py\"\n",
                "    )\n",
                "\n",
                "print(f\"Loading dataset from {dataset_path}...\")\n",
                "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
                "\n",
                "print(f\"‚úÖ Loaded {len(dataset)} examples\")\n",
                "print(f\"\\nSample keys: {dataset.column_names}\")\n",
                "\n",
                "# Apply prompt formatting\n",
                "print(\"\\nApplying prompt template...\")\n",
                "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
                "\n",
                "print(f\"‚úÖ Dataset ready for training ({len(dataset)} examples)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Add LoRA Adapters\n",
                "\n",
                "LoRA allows efficient fine-tuning by updating only 1-10% of parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r=16,  # LoRA rank (higher = more parameters)\n",
                "    target_modules=[\n",
                "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
                "    ],\n",
                "    lora_alpha=16,\n",
                "    lora_dropout=0,  # 0 is optimized\n",
                "    bias=\"none\",\n",
                "    use_gradient_checkpointing=\"unsloth\",  # 30% less VRAM\n",
                "    random_state=3407,\n",
                "    use_rslora=False,\n",
                "    loftq_config=None,\n",
                ")\n",
                "\n",
                "print(\"‚úÖ LoRA adapters added\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Configure Training\n",
                "\n",
                "These settings are optimized for 12GB VRAM."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    train_dataset=dataset,\n",
                "    dataset_text_field=\"text\",\n",
                "    max_seq_length=max_seq_length,\n",
                "    dataset_num_proc=2,\n",
                "    args=TrainingArguments(\n",
                "        per_device_train_batch_size=2,       # Batch size per GPU\n",
                "        gradient_accumulation_steps=4,       # Effective batch = 2*4 = 8\n",
                "        warmup_steps=5,\n",
                "        max_steps=300,                       # Increase for longer training\n",
                "        learning_rate=2e-4,\n",
                "        fp16=not torch.cuda.is_bf16_supported(),  # Use fp16 if bf16 not available\n",
                "        bf16=torch.cuda.is_bf16_supported(),       # Use bf16 on Ampere+ GPUs\n",
                "        logging_steps=10,\n",
                "        optim=\"adamw_8bit\",                  # Saves ~2GB VRAM\n",
                "        weight_decay=0.01,\n",
                "        lr_scheduler_type=\"linear\",\n",
                "        seed=3407,\n",
                "        output_dir=\"outputs\",\n",
                "        save_steps=50,                       # Save checkpoint every 50 steps\n",
                "        save_total_limit=3,                  # Keep only 3 checkpoints\n",
                "    ),\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Trainer configured\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Check GPU Memory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gpu_stats = torch.cuda.get_device_properties(0)\n",
                "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
                "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
                "\n",
                "print(f\"GPU: {gpu_stats.name}\")\n",
                "print(f\"Max memory: {max_memory} GB\")\n",
                "print(f\"Reserved: {start_gpu_memory} GB\")\n",
                "print(f\"Available: {max_memory - start_gpu_memory} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Train!\n",
                "\n",
                "This will take ~30-60 minutes depending on your GPU and dataset size."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üöÄ Starting training...\\n\")\n",
                "\n",
                "trainer_stats = trainer.train()\n",
                "\n",
                "print(\"\\n‚úÖ Training complete!\")\n",
                "print(f\"Final loss: {trainer_stats.training_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Test Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Enable fast inference mode\n",
                "FastLanguageModel.for_inference(model)\n",
                "\n",
                "# Test question\n",
                "test_context = \"\"\"Document 1:\n",
                "NKU tuition for undergraduate students is $450 per credit hour for in-state students and $750 for out-of-state.\n",
                "\n",
                "Document 2:\n",
                "Financial aid applications are available through the FAFSA website.\n",
                "\"\"\"\n",
                "\n",
                "test_prompt = prompt_template.format(\n",
                "    context=test_context,\n",
                "    instruction=\"How much does tuition cost per credit hour?\",\n",
                "    thought=\"\",\n",
                "    answer=\"\"\n",
                ")\n",
                "\n",
                "inputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
                "\n",
                "print(\"Testing model...\\n\")\n",
                "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True, temperature=0.7)\n",
                "response = tokenizer.batch_decode(outputs)[0]\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"RESPONSE:\")\n",
                "print(\"=\" * 60)\n",
                "print(response)\n",
                "print(\"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save Fine-tuned Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save LoRA adapters locally\n",
                "save_dir = \"campus_gpt_lora\"\n",
                "model.save_pretrained(save_dir)\n",
                "tokenizer.save_pretrained(save_dir)\n",
                "\n",
                "print(f\"‚úÖ Model saved to {save_dir}/\")\n",
                "print(\"\\nYou can now:\")\n",
                "print(\"1. Use it with Ollama: Create a Modelfile\")\n",
                "print(\"2. Upload to HuggingFace Hub\")\n",
                "print(\"3. Merge LoRA adapters with base model\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Optional: Merge LoRA with Base Model\n",
                "\n",
                "This creates a standalone model (no adapters needed)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Merge LoRA adapters into base model\n",
                "model.save_pretrained_merged(\n",
                "    \"campus_gpt_merged\",\n",
                "    tokenizer,\n",
                "    save_method=\"merged_16bit\",  # or \"merged_4bit\" for smaller size\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Merged model saved to campus_gpt_merged/\")\n",
                "print(\"This can be used with Ollama or deployed directly.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
